net: "/home/kazeka/src/deep-metric-learning/model/train_val_googlenet_finetune_liftedstructsim_softmax_pair_m128_multilabel_embed128.prototxt"
test_initialization: false
# lr for fine-tuning should be lower than when starting from scratch
base_lr: 0.0001
lr_policy: "step"
gamma: 0.96
# stepsize should also be lower, as we're closer to being done
stepsize: 32000
display: 5
max_iter: 40000
momentum: 0.9
weight_decay: 0.0002
snapshot: 10000
snapshot_prefix: "snapshot_portfolio_embed128_baselr_1E4"
solver_mode: GPU
